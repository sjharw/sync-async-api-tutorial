{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sync vs async methods of making API calls"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the IUCN API to demonstrate synchronous and asynchronous methods of getting API data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get IUCN API key from config.ini\n",
    "from configparser import ConfigParser\n",
    "config = ConfigParser()\n",
    "config.read(\"config.ini\")\n",
    "IUCN_API_KEY = config[\"API\"][\"iucn\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create function for making URL request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we create a function that makes get requets to a url and catches exceptions raised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_url_response(url):\n",
    "    \"\"\"\n",
    "    Makes get request using requests library to specified url.\n",
    "\n",
    "    Args:\n",
    "        url (str): url to make request to\n",
    "\n",
    "    Returns:\n",
    "        response: request response object\n",
    "\n",
    "    Raises:\n",
    "        requests.exceptions.HTTPError (\"Token not valid!\"): Provided token is not accepted\n",
    "        requests.exceptions.HTTPError (HTTPError. Request failed with response status: {response.status_code}, see the following link for more detail on this status: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/{response.status_code})\n",
    "        requests.exceptions.ConnectionError (Connection Error. Error connecting to server url:{url}. Following error raised: {e})\n",
    "        requests.exceptions.RequestException (Request Exception. An error occurred while making the request: {e})\n",
    "    \n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "    except requests.exceptions.ConnectionError as e:\n",
    "        raise requests.exceptions.ConnectionError(f\"Connection Error. Error connecting to server url:{url}. Following error raised: {e}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        raise requests.exceptions.RequestException(f\"An error occurred while making the request: {e}\")\n",
    "    if response.text == '{\"message\":\"Token not valid!\"}':\n",
    "        raise requests.exceptions.HTTPError(\"Token not valid!\")\n",
    "    elif response.status_code != 200:\n",
    "        raise requests.exceptions.HTTPError(f\"HTTPError. Request failed with response status: {response.status_code}, see the following link for more detail on this status: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/{response.status_code}\")\n",
    "    else:\n",
    "        pass\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we test the get_url() function. For testing purposes, you can create a Reponse object using `responses.RequestsMock()` and use that to test functions that make and handle HTTP requests using the `requests` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  pytest, responses, re\n",
    "\n",
    "def test_get_request_sucess():\n",
    "    \"\"\" Test for successful request \"\"\"\n",
    "    # set up response object that gives success status 200\n",
    "    with responses.RequestsMock() as rsps:\n",
    "        rsps.add(responses.GET, 'https://api.example.com/data',\n",
    "                 json={'data': 'example'}, status=200)\n",
    "        response = get_url_response('https://api.example.com/data')\n",
    "        assert response.status_code == 200\n",
    "        assert response.json() == {'data': 'example'}\n",
    "\n",
    "def test_get_request_failed_responsecode():\n",
    "    \"\"\" Test for failed request \"\"\"\n",
    "    with responses.RequestsMock() as rsps:\n",
    "        rsps.add(\n",
    "            responses.GET, \n",
    "            'https://api.example.com/data',\n",
    "            json={'error': 'Request failed'},\n",
    "            status=500\n",
    "        )\n",
    "        with pytest.raises(requests.exceptions.HTTPError) as exc_info:\n",
    "            get_url_response('https://api.example.com/data')\n",
    "        assert str(exc_info.value) == \"HTTPError. Request failed with response status: 500, see the following link for more detail on this status: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500\"\n",
    "\n",
    "def test_get_request_failed_token():\n",
    "    \"\"\" Test for failed request \"\"\"\n",
    "    with responses.RequestsMock() as rsps:\n",
    "        rsps.add(\n",
    "            responses.GET, \n",
    "            'https://api.example.com/data',\n",
    "            body='{\"message\":\"Token not valid!\"}',\n",
    "            content_type='application/json', \n",
    "            status=200\n",
    "        )\n",
    "        with pytest.raises(requests.exceptions.HTTPError) as exc_info:\n",
    "            get_url_response('https://api.example.com/data')\n",
    "        assert str(exc_info.value) == \"Token not valid!\"\n",
    "\n",
    "def test_get_request_failed_url():\n",
    "    \"\"\" Test for failed request \"\"\"\n",
    "    with pytest.raises(requests.exceptions.ConnectionError) as exc_info:\n",
    "        get_url_response('https://api.example.com/invalid-url')\n",
    "    # Get unique \n",
    "    error_message_result = str(exc_info.value)\n",
    "    # Regular expression pattern\n",
    "    pattern = r'0x[0-9a-fA-F]+'\n",
    "    # Search for the pattern in the string\n",
    "    match = re.search(pattern, error_message_result)\n",
    "    error_message_expected = f\"Connection Error. Error connecting to server url:https://api.example.com/invalid-url. Following error raised: HTTPSConnectionPool(host='api.example.com', port=443): Max retries exceeded with url: /invalid-url (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at {match.group()}>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\"\n",
    "    assert error_message_result == error_message_expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_get_request_sucess()\n",
    "test_get_request_failed_responsecode()\n",
    "test_get_request_failed_token()\n",
    "test_get_request_failed_url()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sync methods"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### requests library"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `requests` library makes HTTP requests to a specified URL. You can use the library to set headers, encode and decode data, and handle redirects and authentication. The library will automatically parse the response and return a Response object. The `requests` library only allows for one page request at a time, you cant multiple send requests to run in parallel. The synchronous nature of the `requests` library means you have to wait until a response is recieved before you send the next response. This means the library is very slow when handling multiple requests, but works quickly for making a few requests. Below is a `requests` method for getting API data from a single IUCN page (note IUCN has 16 pages total)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iucn_page(page: int = 0):\n",
    "    \"\"\"\n",
    "    Makes GET request for specific IUCN page.\n",
    "    \"\"\"\n",
    "    url = f\"https://apiv3.iucnredlist.org/api/v3/species/page/{page}?token={IUCN_API_KEY}\"\n",
    "    url_responce = get_url_response(url)\n",
    "    return url_responce.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_multiple_iucn_pages(maximum_pages):\n",
    "    \"\"\" \n",
    "    Makes GET request for pages up to the maximum number of pages.\n",
    "    \"\"\"\n",
    "    [get_iucn_page(page) for page in range(maximum_pages)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Async methods"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asynchronous methods involves making and processing get requests in parallel to speed up large numbers of HTTP requests. It works by sending multiple requests at once (rather than one-by-one) and processing the responses as they are recieved. Here we explore two methods for making API requests asynchronously. The first method uses the `ThreadPoolExecutor` library, and the second method uses the two libraries `asyncio` and `aiohttp`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) ThreadPoolExecutor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ThreadPoolExecutor is a class from the `concurrent.futures` module that allows you to create a pool of worker threads to execute tasks concurrently. Each thread can make and process a HTTP request. You can use it to run multiple get requests (as seperate threads) in parallel by submitting them to the executor and then waiting for the results. Each time ThreadPoolExecutor creates a new thread, there is a small amount of overhead involved, so if the time it takes to make the request is relatively short, this overhead can make the overall execution time longer than using the synchronous requests module. `ThreadPoolExecutor` is optimal when used to make multiple requests that take time to recieve a response for/ process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def get_multiple_iucn_pages_in_parallel(maximum_page: int = 0):\n",
    "    \"\"\" \n",
    "    Gets multiple pages from IUCN API in parallel.\n",
    "    \"\"\"\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = [executor.submit(get_iucn_page, page) for page in range(maximum_page)] # executor.submit() starts the pool of threads\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If you want to access the data from the Future responce\n",
    "# data = get_multiple_iucn_pages_in_parallel(2)\n",
    "# thread_results = []\n",
    "# # Wait for all threads to complete and retrieve the results\n",
    "# for thread in data:\n",
    "#     result = thread.result()  # Get the result from each thread\n",
    "#     thread_results.append(result)\n",
    "# print(thread_results[2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) asyncio & aiohttp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How does asyncio work?**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asyncio uses coroutines which are awaitable functions that work simarly to generators. Generators are objects that contain iterables which, when requested, are computed and stored in memory, while the rest of the iterables are never generated until they are requested. This means you can store an indefinate number of values without using storage space, its not until you call for the value that it then gets stored in memory. Variables and functions, on the other hand, compute all values in advance and store them in memory. Coroutines work like generators, they are functions that arent executed or stored in memory until called. They are declared with the async and await syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator example\n",
    "\n",
    "def inner_generator():\n",
    "    yield 1\n",
    "    yield 2\n",
    "    yield 3\n",
    "\n",
    "def outer_generator():\n",
    "    yield 'a'\n",
    "    yield 'b'\n",
    "    yield 'c'\n",
    "    inner_yield = yield from inner_generator()\n",
    "    print(inner_yield)\n",
    "    yield 'd'\n",
    "    yield 'e'\n",
    "    x = yield\n",
    "    yield x * 3 # send() a value to the geenrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You cant compute and return each variable one at a time in a function\n",
    "\n",
    "# def inner_func():\n",
    "#     return 1\n",
    "#     return 2\n",
    "#     return 3\n",
    "\n",
    "# next(inner_func())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = outer_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'b'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen.send(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coroutines can be paused and resumed, only generating values as needed. They also have the ability to receive values from the caller, simarly to generators. This is particularly important as it means coroutines and be suspended and resumed later, allowing other tasks to run in the meantime. Asyncio allows you to run processes in parallel by running two or more coroutines concurrently."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example of generator vs function**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A generator is very similar to a function that returns an array, in that a generator has parameters, can be called, and generates a sequence of values. However, instead of building an array containing all the values and returning them all at once, a generator yields the values one at a time, which requires less memory and allows the caller to get started processing the first few values immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json, time\n",
    "\n",
    "urls = [\"https://mockapi_cookbook_1\", \"https://mockapi_travel_2\", \"https://mockapi_blog_3\", \"https://mockapi_holidays_4\"]\n",
    "\n",
    "def get_response(url):\n",
    "    \"\"\" Creates mock response object using fake url \"\"\"\n",
    "    info = url.split(\"_\")\n",
    "    response_data = {'id': info[2], 'title': info[1], 'source': url}\n",
    "    response = requests.Response()\n",
    "    response.status_code = 200\n",
    "    response._content = json.dumps(response_data).encode('utf-8')\n",
    "    time.sleep(2)\n",
    "    data = response.json()\n",
    "    return data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you make a get requests using a function, the function would make all the requests, store all the responses in memory, and then returned these responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': '1', 'title': 'cookbook', 'source': 'https://mockapi_cookbook_1'}, {'id': '2', 'title': 'travel', 'source': 'https://mockapi_travel_2'}, {'id': '3', 'title': 'blog', 'source': 'https://mockapi_blog_3'}, {'id': '4', 'title': 'holidays', 'source': 'https://mockapi_holidays_4'}]\n"
     ]
    }
   ],
   "source": [
    "def get_multiple_urls(urls):\n",
    "    \"\"\" Uses list of fake urls to return mock response objects \"\"\"\n",
    "    result = []\n",
    "    for url in urls:\n",
    "        data = get_response(url)\n",
    "        result.append(data)\n",
    "    return result\n",
    "\n",
    "response = get_multiple_urls(urls)\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you make a get request using a generator, you can pause and resume execution of get requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_multiple_urls(urls):\n",
    "    for url in urls:\n",
    "        data = get_response(url)\n",
    "        yield data\n",
    "\n",
    "responses = get_multiple_urls(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '1', 'title': 'cookbook', 'source': 'https://mockapi_cookbook_1'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(responses)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTANT NOTICE: Jupyter creates and runs an event loop in the main thread, thus preventing users from being able to start a loop. To run a async function, you have to add your tasks to the already running Jupyter loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_WindowsSelectorEventLoop running=True closed=False debug=False>\n",
      "{<Task pending name='Task-2' coro=<Kernel.dispatch_queue() running at c:\\Users\\sarah\\anaconda3\\envs\\api-env\\lib\\site-packages\\ipykernel\\kernelbase.py:510> cb=[IOLoop.add_future.<locals>.<lambda>() at c:\\Users\\sarah\\anaconda3\\envs\\api-env\\lib\\site-packages\\tornado\\ioloop.py:687]>}\n",
      "This event loop is already running\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "loop = asyncio.get_event_loop()\n",
    "print(loop)\n",
    "pending = asyncio.all_tasks()\n",
    "print(pending)\n",
    "try:\n",
    "    asyncio.get_event_loop().run_until_complete(asyncio.gather(*asyncio.all_tasks()))\n",
    "except RuntimeError as r:\n",
    "    print(r)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to run an additional event loops for development purposes you can use the [nest-asyncio](https://pypi.org/project/nest-asyncio) package, otherwise, you can simply add your async tasks to the already running event loop."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example of running multiple coroutines**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count one\n",
      "count two\n",
      "count three\n",
      "count five\n",
      "count six\n",
      "count four\n",
      "Script executed in 8.01 seconds.\n"
     ]
    }
   ],
   "source": [
    "import time,asyncio\n",
    "\n",
    "# create tasks\n",
    "async def count():\n",
    "    print(\"count one\")\n",
    "    await asyncio.sleep(8)\n",
    "    print(\"count four\")\n",
    "\n",
    "async def count_further():\n",
    "    print(\"count two\")\n",
    "    await asyncio.sleep(4)\n",
    "    print(\"count five\")\n",
    "\n",
    "async def count_even_further():\n",
    "    print(\"count three\")\n",
    "    await asyncio.sleep(6)\n",
    "    print(\"count six\")\n",
    "\n",
    "async def main():\n",
    "    await asyncio.gather(count(), count_further(), count_even_further()) # gather tasks and await their excecution in the event loop\n",
    "\n",
    "s = time.perf_counter()\n",
    "await main()\n",
    "elapsed = time.perf_counter() - s # time elapsed between starting and ending awaition for async function to finish processing\n",
    "print(f\"Script executed in {elapsed:0.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets use the method described above to create async functions with asyncio and aiohttp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "\n",
    "async def get_iucn_page_asyncio(session, page_number):\n",
    "    url = f\"http://apiv3.iucnredlist.org/api/v3/species/page/{page_number}?token={IUCN_API_KEY}\"\n",
    "    try:\n",
    "        async with session.get(url) as response:\n",
    "            if response.text == '{\"message\":\"Token not valid!\"}':\n",
    "                raise Exception(\"Token not valid!\")\n",
    "            elif response.status == 404:\n",
    "                print(f\"Page {page_number} not found\")\n",
    "            elif response.status != 200:\n",
    "                raise Exception(\n",
    "                    f\"Request failed with response status: {response.status}\"\n",
    "                )\n",
    "            else:\n",
    "                return await response.json()\n",
    "    except aiohttp.ClientError as e:\n",
    "        print(f\"Error occured while getting page {page_number} : {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "async def get_multiple_iucn_pages_asyncio(maximum_page):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = [get_iucn_page_asyncio(session, page_number) for page_number in range(maximum_page)] # create a task for each page request\n",
    "        results = await asyncio.gather(*tasks) # gather tasks and wait for them to process\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare times between methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "def time_function(function_name, pages):\n",
    "    \"\"\" \n",
    "    Time how long it takes to execute a function\n",
    "    that requests multiple pages.\n",
    "    \"\"\"\n",
    "    times = []\n",
    "    for page in pages: # number of pages to request\n",
    "        execution_time = timeit.timeit(lambda: function_name(page), number=1) \n",
    "        print(f\"Execution time for {page} pages: {execution_time:.6f} seconds\") \n",
    "        times.append((page, execution_time))\n",
    "    return times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum_pages = [1, 3, 5, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sync method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time for 1 pages: 2.369900 seconds\n",
      "Execution time for 3 pages: 15.061002 seconds\n",
      "Execution time for 5 pages: 23.790747 seconds\n"
     ]
    }
   ],
   "source": [
    "sync_times = time_function(get_multiple_iucn_pages, maximum_pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Async method using ThreadPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time for 1 pages: 4.712853 seconds\n",
      "Execution time for 3 pages: 7.281000 seconds\n",
      "Execution time for 5 pages: 11.014004 seconds\n"
     ]
    }
   ],
   "source": [
    "async_times = time_function(get_multiple_iucn_pages_in_parallel, maximum_pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Async method using asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time for 1 pages: 2.74 seconds.\n",
      "Execution time for 3 pages: 8.26 seconds.\n",
      "Execution time for 5 pages: 11.51 seconds.\n"
     ]
    }
   ],
   "source": [
    "for max_page in maximum_pages:\n",
    "    s = time.perf_counter()\n",
    "    await get_multiple_iucn_pages_asyncio(max_page)\n",
    "    elapsed = time.perf_counter() - s\n",
    "    print(f\"Execution time for {max_page} pages: {elapsed:0.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_results(times):\n",
    "    # x axis values\n",
    "    x = [i[0] for i in times] # page number\n",
    "    # corresponding y axis values\n",
    "    y = [i[1] for i in times] # execution time\n",
    "    \n",
    "    # plotting the points \n",
    "    plt.plot(x, y)\n",
    "    \n",
    "    # naming the x axis\n",
    "    plt.xlabel('page number')\n",
    "    # naming the y axis\n",
    "    plt.ylabel('execution time (sec)')\n",
    "    \n",
    "    # giving a title to my graph\n",
    "    plt.title('Increasing synchronous requests')\n",
    "    \n",
    "    # function to show the plot\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot showing that the time taken to make multiple get requests with the `requests` library increases linearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sync = plot_results(sync_times)\n",
    "plot_sync.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "api-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "923019c6e1ca6f51402cf574cb88c838ae6a179377b5dd302271172441e3c0c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
