{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API calls- sync vs async"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the IUCN API to demonstrate synchronous and asynchronous methods of getting API data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup paths and libraries\n",
    "import os\n",
    "import sys\n",
    "ROOT = os.path.dirname(os.path.abspath(\"api\"))\n",
    "if ROOT not in sys.path:\n",
    "   sys.path.append(ROOT)\n",
    "\n",
    "# API key from config.ini\n",
    "from configparser import ConfigParser\n",
    "config = ConfigParser()\n",
    "config.read(ROOT + \"/config.ini\")\n",
    "IUCN_API_KEY = config[\"API\"][\"iucn\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sync methods"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### requests library"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `requests` library makes HTTP requests to a specified URL. You can use the library to set headers, encode and decode data, and handle redirects and authentication. The library will automatically parse the response and return a Response object. The `requests` library only allows for one page request at a time, you cant multiple send requests to run in parallel. The synchronous nature of the `requests` library means you have to wait until a response is recieved before you send the next response. This means the library is very slow when handling multiple requests, but works quickly for making a few requests. Below is a `requests` method for getting API data from IUCN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json\n",
    "\n",
    "def check_response(response):\n",
    "    \"\"\" \n",
    "    Checks the response of a request to see if it is valid.\n",
    "\n",
    "    Raises:\n",
    "        Exception (\"Token not valid!\"): Provided token is not accepted\n",
    "        Exception (Request failed with response status: {response.status_code}): Response failed\n",
    "    \"\"\"\n",
    "    if response.text == '{\"message\":\"Token not valid!\"}':\n",
    "        raise Exception(\"Token not valid!\")\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Request failed with response status: {response.status_code}\")\n",
    "\n",
    "def get_iucn_page(page: int = 0):\n",
    "    \"\"\" \n",
    "    Makes get request using requests library to IUCN url\n",
    "    for specified page number.\n",
    "\n",
    "    Args:\n",
    "        page (int): page to make get request for\n",
    "\n",
    "    Returns:\n",
    "        response: request response object\n",
    "    \"\"\"\n",
    "    url = f\"https://apiv3.iucnredlist.org/api/v3/species/page/{page}?token={IUCN_API_KEY}\"\n",
    "    response = requests.get(url)\n",
    "    # error handling\n",
    "    check_response(response)\n",
    "    return response\n",
    "\n",
    "def get_all_iucn_pages(pages):\n",
    "    \"\"\"\n",
    "    Requests multiple pages from IUCN endpoint.\n",
    "\n",
    "    Args:\n",
    "        pages(int): all the pages to request\n",
    "\n",
    "    Returns:\n",
    "        results (list): list of dicttionaries containing IUCN json response\n",
    "    \"\"\"\n",
    "    page = 0\n",
    "    results = []\n",
    "    while page < pages:\n",
    "        response = get_iucn_page(page)\n",
    "        resp_dict = json.loads(response.text)\n",
    "        page = int(resp_dict[\"page\"]) + 1\n",
    "        # add to results\n",
    "        results.extend(resp_dict[\"result\"])\n",
    "    return results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test the function**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For testing purposes, you can create a Reponse object using `responses.RequestsMock()` and use that to test functions that make and handle HTTP requests using the `requests` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, pytest, responses\n",
    "\n",
    "def get_url(url: str, headers: dict = {}):\n",
    "    \"\"\" \n",
    "    Example function for making a GET request and throwing and error if the request fails.\n",
    "    \"\"\"\n",
    "    response = requests.get(url, headers)\n",
    "    check_response(response) # use previously defined function for handling request responce\n",
    "    return response\n",
    "\n",
    "def test_get_request_sucess():\n",
    "    \"\"\" Test for successful request \"\"\"\n",
    "    # set up response object that gives success status 200\n",
    "    with responses.RequestsMock() as rsps:\n",
    "        rsps.add(responses.GET, 'https://api.example.com/data',\n",
    "                 json={'data': 'example'}, status=200)\n",
    "        headers = {\"Authorization\": \"Token abcdefg\"}\n",
    "        response = get_url('https://api.example.com/data', headers)\n",
    "        assert response.status_code == 200\n",
    "        assert response.json() == {'data': 'example'}\n",
    "\n",
    "def test_get_request_failed():\n",
    "    \"\"\" Test for failed request \"\"\"\n",
    "    with responses.RequestsMock() as rsps:\n",
    "        rsps.add(responses.GET, 'https://api.example.com/data',\n",
    "                 json={'error': 'Request failed'}, status=401)\n",
    "\n",
    "        headers = {}\n",
    "        with pytest.raises(Exception) as exc_info:\n",
    "            get_url('https://api.example.com/data', headers)\n",
    "        assert str(exc_info.value) == \"Request failed with response status: 401\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_get_request_sucess()\n",
    "test_get_request_failed()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Time the HTTP request**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "times = []\n",
    "for i in [1, 5, 10, 15]: # number of pages to request\n",
    "    execution_time = timeit.timeit(lambda: get_all_iucn_pages(i), number=1) \n",
    "    print(f\"Execution time for {i} pages: {execution_time:.6f} seconds\") \n",
    "    times.append((i, execution_time))\n",
    "\n",
    "print(times)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot showing that the time taken to make multiple get requests with the `requests` library increases expodentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "  \n",
    "# x axis values\n",
    "x = [i[0] for i in times] # page number\n",
    "# corresponding y axis values\n",
    "y = [i[1] for i in times] # execution time\n",
    "  \n",
    "# plotting the points \n",
    "plt.plot(x, y)\n",
    "  \n",
    "# naming the x axis\n",
    "plt.xlabel('page number')\n",
    "# naming the y axis\n",
    "plt.ylabel('execution time (sec)')\n",
    "  \n",
    "# giving a title to my graph\n",
    "plt.title('Increasing synchronous requests')\n",
    "  \n",
    "# function to show the plot\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Async methods"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asynchronous methods involves making and processing get requests in parallel to speed up large numbers of HTTP requests. It works by sending multiple requests at once (rather than one-by-one) and processing the responses as they are recieved. Here we explore two methods for making API requests asynchronously. The first method uses the `ThreadPoolExecutor` library, and the second method uses the two libraries `asyncio` and `aiohttp`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) ThreadPoolExecutor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ThreadPoolExecutor is a class from the `concurrent.futures` module that allows you to create a pool of worker threads to execute tasks concurrently. Each thread can make and process a HTTP request. You can use it to run multiple get requests (as seperate threads) in parallel by submitting them to the executor and then waiting for the results. Each time ThreadPoolExecutor creates a new thread, there is a small amount of overhead involved, so if the time it takes to make the request is relatively short, this overhead can make the overall execution time longer than using the synchronous requests module. `ThreadPoolExecutor` is optimal when used to make multiple requests that take time to recieve a response for/ process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "pages = range(1, 10)\n",
    "\n",
    "def get_iucn_pages_in_parallel(pages):\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = [executor.submit(get_iucn_page, page_number) for page_number in pages] # executor.submit() starts the pool of threads\n",
    "        for result in results:\n",
    "            data = result.result()\n",
    "            return data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time the HTTP request with the timeit module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "execution_time = timeit.timeit(lambda: get_iucn_pages_in_parallel(pages), number=1) \n",
    "print(f\"Execution time for {max(pages)} pages: {execution_time:.6f} seconds\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) asyncio & aiohttp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How does asyncio work?**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asyncio uses coroutines which are awaitable functions that work simarly to generators. Generators are objects that contain iterables which, when requested, are computed and stored in memory, while the rest of the iterables are never generated until they are requested. This means you can store an indefinate number of values without using storage space, its not until you call for the value that it then gets stored in memory. Variables and functions, on the other hand, compute all values in advance and store them in memory. Coroutines work like generators, they are functions that arent executed or stored in memory until called. They are declared with the async and await syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator example\n",
    "\n",
    "def inner_generator():\n",
    "    yield 1\n",
    "    yield 2\n",
    "    yield 3\n",
    "\n",
    "def outer_generator():\n",
    "    yield 'a'\n",
    "    yield 'b'\n",
    "    yield 'c'\n",
    "    inner_yield = yield from inner_generator()\n",
    "    print(inner_yield)\n",
    "    yield 'd'\n",
    "    yield 'e'\n",
    "    x = yield\n",
    "    yield x * 3 # send() a value to the geenrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You cant compute and return each variable one at a time in a function\n",
    "\n",
    "def inner_func():\n",
    "    return 1\n",
    "    return 2\n",
    "    return 3\n",
    "\n",
    "next(inner_func())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = outer_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen.send(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coroutines can be paused and resumed, only generating values as needed. They also have the ability to receive values from the caller, simarly to generators. This is particularly important as it means coroutines and be suspended and resumed later, allowing other tasks to run in the meantime. Asyncio allows you to run processes in parallel by running two or more coroutines concurrently."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example of generator vs function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json, time\n",
    "\n",
    "urls = [\"https://mockapi_cookbook_1\", \"https://mockapi_travel_2\", \"https://mockapi_blog_3\", \"https://mockapi_holidays_4\"]\n",
    "\n",
    "def get_response(url):\n",
    "    \"\"\" Creates mock response object using fake url \"\"\"\n",
    "    info = url.split(\"_\")\n",
    "    response_data = {'id': info[2], 'title': info[1], 'source': url}\n",
    "    response = requests.Response()\n",
    "    response.status_code = 200\n",
    "    response._content = json.dumps(response_data).encode('utf-8')\n",
    "    time.sleep(2)\n",
    "    data = response.json()\n",
    "    return data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you make a get requests using a function, the function would make all the requests, store all the responses, and then returned these responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'source': 'https://mockapi_1', 'title': 'Mock Title', 'body': 'Mock Body'},\n",
       " {'source': 'https://mockapi_2', 'title': 'Mock Title', 'body': 'Mock Body'},\n",
       " {'source': 'https://mockapi_3', 'title': 'Mock Title', 'body': 'Mock Body'},\n",
       " {'source': 'https://mockapi_4', 'title': 'Mock Title', 'body': 'Mock Body'}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_multiple_urls(urls):\n",
    "    \"\"\" Uses list of fake urls to return mock response objects \"\"\"\n",
    "    for url in urls:\n",
    "        data = get_response(url)\n",
    "        return data\n",
    "\n",
    "[get_url(url) for url in urls]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you make a get request using a generator, you can pause and resume the get requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_multiple_urls(urls):\n",
    "    for url in urls:\n",
    "        data = get_response(url)\n",
    "        yield data\n",
    "\n",
    "responses = get_multiple_urls(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '3', 'title': 'blog', 'source': 'https://mockapi_blog_3'}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(responses)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTANT NOTICE: Jupyter creates and runs an event loop in the main thread, thus preventing users from being able to start a loop. To run a async function, you have to add your tasks to the already running Jupyter loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_WindowsSelectorEventLoop running=True closed=False debug=False>\n",
      "{<Task pending name='Task-3' coro=<Kernel.dispatch_queue() running at c:\\Users\\sarah\\anaconda3\\envs\\api-env\\lib\\site-packages\\ipykernel\\kernelbase.py:510> cb=[IOLoop.add_future.<locals>.<lambda>() at c:\\Users\\sarah\\anaconda3\\envs\\api-env\\lib\\site-packages\\tornado\\ioloop.py:687, gather.<locals>._done_callback() at c:\\Users\\sarah\\anaconda3\\envs\\api-env\\lib\\asyncio\\tasks.py:720]>}\n",
      "This event loop is already running\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "loop = asyncio.get_event_loop()\n",
    "print(loop)\n",
    "pending = asyncio.all_tasks()\n",
    "print(pending)\n",
    "try:\n",
    "    asyncio.get_event_loop().run_until_complete(asyncio.gather(*asyncio.all_tasks()))\n",
    "except RuntimeError as r:\n",
    "    print(r)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to run an additional event loops for development purposes you can use the [nest-asyncio](https://pypi.org/project/nest-asyncio) package, otherwise, you can simply add your async tasks to the already running event loop."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example of running multiple coroutines**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count one\n",
      "count two\n",
      "count three\n",
      "count five\n",
      "count six\n",
      "count four\n",
      "Script executed in 8.00 seconds.\n"
     ]
    }
   ],
   "source": [
    "import time,asyncio\n",
    "\n",
    "# create tasks\n",
    "async def count():\n",
    "    print(\"count one\")\n",
    "    await asyncio.sleep(8)\n",
    "    print(\"count four\")\n",
    "\n",
    "async def count_further():\n",
    "    print(\"count two\")\n",
    "    await asyncio.sleep(4)\n",
    "    print(\"count five\")\n",
    "\n",
    "async def count_even_further():\n",
    "    print(\"count three\")\n",
    "    await asyncio.sleep(6)\n",
    "    print(\"count six\")\n",
    "\n",
    "async def main():\n",
    "    await asyncio.gather(count(), count_further(), count_even_further()) # gather tasks and await their excecution in the event loop\n",
    "\n",
    "s = time.perf_counter()\n",
    "await main()\n",
    "elapsed = time.perf_counter() - s # time elapsed between starting and ending awaition for async function to finish processing\n",
    "print(f\"Script executed in {elapsed:0.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "\n",
    "async def get_iucn_page(session, page_number):\n",
    "    url = f\"http://apiv3.iucnredlist.org/api/v3/species/page/{page_number}?token={IUCN_API_KEY}\"\n",
    "    try:\n",
    "        async with session.get(url) as response:\n",
    "            if response.text == '{\"message\":\"Token not valid!\"}':\n",
    "                raise Exception(\"Token not valid!\")\n",
    "            elif response.status == 404:\n",
    "                print(f\"Page {page_number} not found\")\n",
    "            elif response.status != 200:\n",
    "                raise Exception(\n",
    "                    f\"Request failed with response status: {response.status}\"\n",
    "                )\n",
    "            else:\n",
    "                return await response.json()\n",
    "    except aiohttp.ClientError as e:\n",
    "        print(f\"Error occured while getting page {page_number} : {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "async def get_multiple_iucn_pages(page_numbers):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = [get_iucn_page(session, page_number) for page_number in page_numbers] # create a task for each page request\n",
    "        results = await asyncio.gather(*tasks) # gather tasks and wait for them to process\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script executed in 18.29 seconds.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "s = time.perf_counter()\n",
    "await get_multiple_iucn_pages(range(0,2))\n",
    "elapsed = time.perf_counter() - s\n",
    "print(f\"Script executed in {elapsed:0.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time = timeit.timeit(await get_multiple_iucn_pages(range(0, 2)))\n",
    "\n",
    "# print(f\"Time taken: {time:.6f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "api-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "923019c6e1ca6f51402cf574cb88c838ae6a179377b5dd302271172441e3c0c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
